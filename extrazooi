import urllib
import urllib.request
import re
from re import findall
from scipy.sparse import coo_matrix
import numpy as np

# Links uit het textbestand "Links" wordt opgeslagen in een lijst
GelezenLinks = open('Links').read()
Links = (GelezenLinks[2:-2]).split("', '")

# Relevanties uit het textbestand "Pagerank" wordt opgeslagen in een lijst
GelezenPagerank = open('Pagerank').read()
GelezenPagerank = (GelezenPagerank[2:-1]).split()
Pagerank=[]
for i in GelezenPagerank:
    Pagerank.append(float(i))
    
def zoek(woord): # De zoekfunctie
    for i in range(len(Links)):
        InhoudBestand=open(str(i)).read() #Opent de opgeslagen broncode van link i
        if woord.lower() in InhoudBestand.lower():
            # zoek eerst titel op
            try:
                titel = re.search(r'<title>(.*)</title>', InhoudBestand)
                # kijk of woord in de titel zit
                if woord.lower() in titel.group(1).lower():
                    Pagerank[i]=Pagerank[i]*2 # Als woord in titel dan wordt pagerank vedubbeld
            except:
                pass
            # zoek hoe vaak het woord voorkomt, pagerank wordt met dat aantal vermenigvuldigd.
            aantal = InhoudBestand.count(woord)
            Pagerank[i]=Pagerank[i]*aantal
            
            Woorderin.append([i,Links[i],Pagerank[i]]) # Lijst met ("index", "link die het trefwoord bevat", "pagerank van die link")
            
        else:
            pass

while True: # Als het programma wordt gerund, wordt zoekwoord opgevraagd en zoekfunctie aangeroepen.
    woord = input("Welk woord wil je zoeken? (Als je klaar bent met zoeken, typ dan 'exit')\n")
    if woord == "exit": #Zoektocht wordt afgebroken na typen "exit"
        print("Bedankt voor het gebruiken van onze superzoekmachine.")
        break
    else: # Zolang niet "exit" wordt getypt, kan je nieuwe zoekwoorden opgeven.
        Woorderin = []
        zoek(woord)
        Woorderin=sorted(Woorderin, key=lambda x: -1*x[2]) # functie om op pagerank te sorteren, -1* om van groot naar klein te sorteren
        if len(Woorderin)==0:
            print("Geen resultaten")
        else:
            for i in Woorderin:
                print(i[1])



import urllib
import urllib.request
import re
from re import findall
from scipy.sparse import coo_matrix
import numpy as np

domein = 'madurodam.nl'
Lijst=['http://www.madurodam.nl/']

def Schrijffunctie(myurl): # Functie die alle links van de domeinnaam "myurl" opslaat in een lijst
    try:
        x = urllib.request.urlopen(myurl) # De webpagina wordt opgehaald
        for i in re.findall('''href=["'](.[^"']+)["']''', str(x.read()), re.I): # Zoekt naar alle aanwezige links in de broncode
            if re.search("css|png$|ico$|/rss/|.pdf$|facebook|twitter|mailto|google| |jpg|.doc", i) == None: # Checkt of de gevonden links niet een plaatje, facebookverwijzing of iets vervelends zijn.
                if (re.search(domein, i) == None) and (re.match('www', i)==None) and (re.match('http', i) == None): # Als de gevonden link alleen een extensie bevat, wordt de domeinnaam ervoor geplakt
                    if re.match("/", i) == None: # Indien nodig wordt nog een / voor de extensie geplakt
                        i = "/" + i
                    i = "http://www." + domein + i
                    if (i in Lijst) == False: # De link wordt aan de de lijst Links toegevoegd als deze er nog niet in staat.
                        Lijst.append(i)
                        
                elif re.search(domein, i): # Als de gevonden link wel de domein voor de extensie heeft staan hoeft die er niet meer voorgeplakt te worden.
                    if (i in Lijst) == False: # De link wordt aan de de lijst Links toegevoegd als deze er nog niet in staat.
                        Lijst.append(i)
    except:
        pass

for i in Lijst:  # Alle pagina's worden doorlopen en de links opgeslagen in list "Lijst"
    Schrijffunctie(i)

for i in range(len(Lijst)): # De broncode van iedere pagina wordt opgeslagen in een text bestand.
    try: # Als de link te openen is, wordt de broncode in een bestand opgeslagen
        text = open(str(i),'w')
        text.write(str(urllib.request.urlopen(Lijst[i]).read()))
    except: # Als de link niet te openen is, wordt het bestand leeg gelaten
        text2 = open(str(i),'w')
        
        
import urllib
import urllib.request
import re
from re import findall
from scipy.sparse import coo_matrix
import numpy as np


X=[] # lijst met 2D-vectoren ("link waarnaar verwezen wordt","link waar vanuit verwezen wordt")
row=[] # lijst met niet-ijle rij-indices voor de PageRank matrix
col=[] # lijst met niet-ijle kolom-indices voor de PageRank matrix
domein = 'madurodam.nl'
Links=['http://www.madurodam.nl/'] # lijst van interne links die in de domein voorkomen

def crawler(myurl): # De webcrawl-functie.
    try:
        x = urllib.request.urlopen(myurl) # De webpagina wordt opgehaald
        for i in re.findall('''href=["'](.[^"']+)["']''', str(x.read()), re.I): # Zoekt naar alle aanwezige links in de broncode
            if re.search("css|png$|ico$|/rss/|.pdf$|facebook|twitter|mailto|google| |jpg|.doc", i) == None: # Checkt of de gevonden links niet een plaatje, facebookverwijzing of iets vervelends zijn.
                if (re.search(domein, i) == None) and (re.match('www', i)==None) and (re.match('http', i) == None): # Als de gevonden link alleen een extensie bevat, wordt de domeinnaam ervoor geplakt
                    if re.match("/", i) == None:
                        i = "/" + i # Indien nodig wordt nog een / voor de extensie geplakt
                    i = "http://www." + domein + i
                    if (i in Links) == False: # De link wordt aan de de lijst Links toegevoegd als deze er nog niet in staat.
                        Links.append(i)
                    a = (Links.index(i),Links.index(myurl)) # 2D vector ("link waarnaar verwezen wordt","link waar vanuit verwezen wordt")
                    if (a in X) == False: # indien de verwijzing nieuw is worden nieuwe waardes aan de vector-lijst, rij-lijst en kolom-lijst toegevoegd
                        X.append(a)
                        row.append(Links.index(i))
                        col.append(Links.index(myurl))
                        
                elif re.search(domein, i): # Als de gevonden link wel de domein voor de extensie heeft staan hoeft die er niet meer voorgeplakt te worden.
                    
                    if (i in Links) == False: # De link wordt aan de de lijst Links toegevoegd als deze er nog niet in staat.
                        Links.append(i)
                    a = (Links.index(i),Links.index(myurl)) # 2D vector ("link waarnaar verwezen wordt","link waar vanuit verwezen wordt")
                    if (a in X) == False: # indien de verwijzing nieuw is worden nieuwe waardes aan de vector-lijst, rij-lijst en kolom-lijst toegevoegd
                        X.append(a)
                        row.append(Links.index(i))
                        col.append(Links.index(myurl))
                        
    except:
        pass
    
crawler('http://www.madurodam.nl/') # Eerst behandelt de crawler de hoofdpagina

for i in Links: # De crawler doorloopt de rest van de pagina's
    crawler(i)
    
data = len(col)*[1] # lijst enen. Samen met de indices-lijsten Col en Row kan hiermee de matrix gemaakt worden.
for i in range(len(col)): # data-lijst wordt per kolom genormeerd.
    deel = col.count(col[i])
    if deel != 0:
        data[i] = 1/deel
        
n = len(Links)
pagerank = n*[1/n] # Genormeerde vector die de PageRank gaat bepalen na k vermenigvuldigingen met de matrix.
matrix = coo_matrix((data, (row, col)), shape=(n,n)).toarray() # De PageRank Matrix

a = 0.15
s = (n,n)
P = np.ones(s)

for i in range(10): # Het PageRank algoritme
    v1=(1-a)*matrix.dot(pagerank)
    v2 =(a/n)*P.dot(pagerank)
    pagerank = v1+v2

text1 = open('Pagerank','w') # Lijst met relevanties per pagina wordt weggeschreven naar textbestand "Pagerank"
text1.write(str(pagerank))

text2 = open('Links','w') # Lijst met links wordt weggeschreven naar textbestand "Links"
text2.write(str(Links))



